{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfdb23531c3d406ebd3461fb6b5c8372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiming/anaconda3/envs/DiT/lib/python3.11/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "The config attributes {'center_input_sample': False, 'flip_sin_to_cos': True, 'freq_shift': 0, 'mid_block_type': 'UNetMidBlock2DCrossAttn', 'only_cross_attention': False, 'attention_head_dim': 8, 'dual_cross_attention': False, 'class_embed_type': None, 'addition_embed_type': None, 'num_class_embeds': None, 'upcast_attention': False, 'resnet_time_scale_shift': 'default', 'resnet_skip_time_act': False, 'resnet_out_scale_factor': 1.0, 'time_embedding_type': 'positional', 'time_embedding_dim': None, 'time_embedding_act_fn': None, 'timestep_post_act': None, 'time_cond_proj_dim': None, 'conv_in_kernel': 3, 'conv_out_kernel': 3, 'projection_class_embeddings_input_dim': None, 'class_embeddings_concat': False, 'mid_block_only_cross_attention': None, 'cross_attention_norm': None, 'addition_embed_type_num_heads': 64} were passed to UNetMotionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c081d0c9937411fa3e99e7dc243eece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'animation1.gif'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import AnimateDiffPipeline, MotionAdapter, EulerDiscreteScheduler\n",
    "from diffusers.utils import export_to_gif\n",
    "from huggingface_hub import hf_hub_download\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "device = \"cuda:9\"\n",
    "dtype = torch.float16\n",
    "\n",
    "step = 4  # Options: [1,2,4,8]\n",
    "repo = \"ByteDance/AnimateDiff-Lightning\"\n",
    "ckpt = f\"animatediff_lightning_{step}step_diffusers.safetensors\"\n",
    "base = \"emilianJR/epiCRealism\"  # Choose to your favorite base model.\n",
    "\n",
    "adapter = MotionAdapter().to(device, dtype)\n",
    "# adapter.load_state_dict(load_file(hf_hub_download(repo ,ckpt), device=device))\n",
    "adapter.load_state_dict(load_file(f'/home/yiming/project/Acceleration/AnimateDiff-Lightning/AnimateDiff-Lightning/{ckpt}', device=device))\n",
    "pipe = AnimateDiffPipeline.from_pretrained(base, motion_adapter=adapter, torch_dtype=dtype).to(device)\n",
    "pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\", beta_schedule=\"linear\")\n",
    "\n",
    "output = pipe(prompt=\"A quiet beach at dawn, the waves gently lapping at the shore and the sky painted in pastel hues.\", guidance_scale=1.0, num_inference_steps=step)\n",
    "export_to_gif(output.frames[0], \"animation1.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f52af7c179f64566b8d59b048e4ba654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ce211473dd494cbd4d9a64990dfe08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dbb1683bc804d40b67a48831092e67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9bd1ee549c452883c6b60cdea0c1ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba281ba08414b5f88cc1c956a83adad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e26630a9aa940c8a12326769bb8e586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448794cfa0384effa628634e6e10e13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329281a3ff5c45a2a27765d9bb586f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4409ef1a0504198a0764938f0ef812b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompts = [\"Sunset over the sea.\",\n",
    "           \"A quiet beach at dawn, the waves gently lapping at the shore and the sky painted in pastel hues.\",\n",
    "           \"Time-lapse of a coastal landscape transitioning from sunrise to nightfall.\",\n",
    "           \"A serene underwater scene featuring a sea turtle swimming.\",\n",
    "           \"Yellow and black tropical fish dart through the sea.\",\n",
    "           \"a dynamic interaction between the ocean and a large rock.\",\n",
    "           \"The dynamic movement of tall, wispy grasses swaying in the wind.\",\n",
    "           \"Slow pan upward of blazing oak fire in an indoor fireplace.\",\n",
    "           \"A serene waterfall cascading down moss-covered rocks.\"]\n",
    "for prompt in prompts:\n",
    "    output = pipe(prompt=prompt, guidance_scale=1.0, num_inference_steps=step)\n",
    "    export_to_gif(output.frames[0], \"{}.gif\".format(prompt[:30].replace(\" \", \"_\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'conv_in_channels': None} were passed to MotionAdapter, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f800b3b6a0214936ad43ccae06fcf61a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'center_input_sample': False, 'flip_sin_to_cos': True, 'freq_shift': 0, 'mid_block_type': 'UNetMidBlock2DCrossAttn', 'only_cross_attention': False, 'attention_head_dim': 8, 'dual_cross_attention': False, 'class_embed_type': None, 'addition_embed_type': None, 'num_class_embeds': None, 'upcast_attention': False, 'resnet_time_scale_shift': 'default', 'resnet_skip_time_act': False, 'resnet_out_scale_factor': 1.0, 'time_embedding_type': 'positional', 'time_embedding_dim': None, 'time_embedding_act_fn': None, 'timestep_post_act': None, 'time_cond_proj_dim': None, 'conv_in_kernel': 3, 'conv_out_kernel': 3, 'projection_class_embeddings_input_dim': None, 'class_embeddings_concat': False, 'mid_block_only_cross_attention': None, 'cross_attention_norm': None, 'addition_embed_type_num_heads': 64} were passed to UNetMotionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2173c9213684062a6209b63bb66fad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e082614e122c4d9f81266286c6b900c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7ae9b08d71490bab5987a4140354c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b22424e87b40829cc77e11302fe394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88810da4b2794f47adcc01e762608c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9866aac955ec44aab47dcbbad4682bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9deef29efe7a4b3f85408dfab73b0826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6484a02fd0a1444f9a10020e610e9850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe25af8f7e64e10b1ecdc8b0d37d582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import AnimateDiffPipeline, LCMScheduler, MotionAdapter\n",
    "from diffusers.utils import export_to_gif\n",
    "\n",
    "adapter = MotionAdapter.from_pretrained(\"wangfuyun/AnimateLCM\", torch_dtype=torch.float16)\n",
    "pipe = AnimateDiffPipeline.from_pretrained(\"emilianJR/epiCRealism\", motion_adapter=adapter, torch_dtype=torch.float16)\n",
    "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config, beta_schedule=\"linear\")\n",
    "\n",
    "pipe.load_lora_weights(\"wangfuyun/AnimateLCM\", weight_name=\"AnimateLCM_sd15_t2v_lora.safetensors\", adapter_name=\"lcm-lora\")\n",
    "# pipe.set_adapters([\"lcm-lora\"], [0.8])\n",
    "\n",
    "pipe.enable_vae_slicing()\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "prompts = [\"Sunset over the sea.\",\n",
    "           \"A quiet beach at dawn, the waves gently lapping at the shore and the sky painted in pastel hues.\",\n",
    "           \"Time-lapse of a coastal landscape transitioning from sunrise to nightfall.\",\n",
    "           \"A serene underwater scene featuring a sea turtle swimming.\",\n",
    "           \"Yellow and black tropical fish dart through the sea.\",\n",
    "           \"a dynamic interaction between the ocean and a large rock.\",\n",
    "           \"The dynamic movement of tall, wispy grasses swaying in the wind.\",\n",
    "           \"Slow pan upward of blazing oak fire in an indoor fireplace.\",\n",
    "           \"A serene waterfall cascading down moss-covered rocks.\"]\n",
    "for prompt in prompts:\n",
    "    output = pipe(prompt=prompt,\n",
    "                  negative_prompt=\"bad quality, worse quality, low resolution\",\n",
    "                    num_frames=16,\n",
    "                    guidance_scale=2.0,\n",
    "                    num_inference_steps=6,\n",
    "                    generator=torch.Generator(\"cpu\").manual_seed(0),\n",
    "                )\n",
    "    export_to_gif(output.frames[0], \"AnimateLCM_{}.gif\".format(prompt[:30].replace(\" \", \"_\")))\n",
    "    \n",
    "# output = pipe(\n",
    "#     prompt=\"A space rocket with trails of smoke behind it launching into space from the desert, 4k, high resolution\",\n",
    "#     negative_prompt=\"bad quality, worse quality, low resolution\",\n",
    "#     num_frames=16,\n",
    "#     guidance_scale=2.0,\n",
    "#     num_inference_steps=6,\n",
    "#     generator=torch.Generator(\"cpu\").manual_seed(0),\n",
    "# )\n",
    "# frames = output.frames[0]\n",
    "# export_to_gif(frames, \"animatelcm.gif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DiT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
